{"cells":[{"cell_type":"code","source":["!pip install relevanceai\n","!pip install vectorhub\n","!pip install transformers\n","!pip install sentence-transformers"],"metadata":{"id":"OlIeShoU0H3q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ME7v2rEL0HFD"},"source":["## ðŸŽ° set parameters"]},{"cell_type":"markdown","metadata":{"id":"IWt6NbOh0HFG"},"source":["To create your clustering application, I will first import my data, preprocess it and encode a part of it.\n","After hacing uploaded it to relevanceai, I will apply a clustering algorithm to create a sharable application.\n","The following paramters will be used for the entire project."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"_sYJu5K_0HFH","executionInfo":{"status":"ok","timestamp":1645623309472,"user_tz":-60,"elapsed":866,"user":{"displayName":"Michelangiolo Mazzeschi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11600464690802347668"}}},"outputs":[],"source":["PROJECT_ID = '<project-name>'\n","API_KEY = '<api-key>'\n","ENCODING_FIELDS = ['text'] #they need to be a list\n","DATASET_NAME = 'steam_reviews' #name of the dataset file in local, just for reference\n","DATASET_ID = 'steam-reviews' #name of the dataset_id on relevanceai\n","MODEL = 'all-MiniLM-L6-v2'\n","VECTOR_SUFFIX = '_sentence_transformers_vector_'\n","FIELDS = ['text']\n","CLUSTERS = 240\n","AGG = {}"]},{"cell_type":"markdown","metadata":{"id":"z9KOVzLB0HFI"},"source":["## ðŸ… relevanceai login"]},{"cell_type":"markdown","metadata":{"id":"pSgiUisG0HFJ"},"source":["To login in relevanceai you will need a \n","Once you initiate a client, you will be able to send and edit data on our servers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W2ZpiejS0HFJ"},"outputs":[],"source":["#relevanceai client\n","import relevanceai\n","\n","client = relevanceai.Client(PROJECT_ID, API_KEY)"]},{"cell_type":"markdown","metadata":{"id":"qUT_g7U30HFK"},"source":["## ðŸ§¶ preparing the data"]},{"cell_type":"markdown","metadata":{"id":"Z-taUbVh0HFK"},"source":["Most of the preprocessing work is done on a pandas DataFrame. \n","However, because this format cannot be directly uploaded to relevanceai, we will need to convert our DataFrame into a list of dictionaries and then upload it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TA0hliIO0HFL"},"outputs":[],"source":["import nltk\n","import pandas as pd\n","import numpy as np\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stopwords = nltk.corpus.stopwords.words('english')\n","from vectorhub.encoders.text.sentence_transformers import SentenceTransformer2Vec\n","from sklearn.metrics.pairwise import euclidean_distances\n","\n","model = SentenceTransformer2Vec(MODEL)\n","\n","def zeroshot(df_text, df_text_vectors, model, top_common, top_sample):\n","    #df_text is the list of text\n","    #df_text is the list of vecotrized text\n","\n","    #tokenize all words\n","    all_words = []\n","    for t in df_text:\n","        all_words += nltk.tokenize.word_tokenize(t)\n","    all_words\n","\n","    #frequency dictionary\n","    all_words_dist = nltk.FreqDist(w.lower() for w in all_words)\n","    all_words_except_stop_dist = nltk.FreqDist(w.lower() for w in all_words if w not in stopwords and w.isalnum() and len(w) != 1)\n","\n","    #dictionary of vectorized top frequent words\n","    dictionary_words = [{\"_id\": i,\"label\": w[0], \"label_vector_\": model.encode(w[0])} for i, w in enumerate(all_words_except_stop_dist.most_common(top_common))]\n","\n","    #\n","    closest_topn_index = np.argsort(euclidean_distances(\n","        [d for d in df_text_vectors], \n","        np.array([vectorized_word[\"label_vector_\"] for vectorized_word in dictionary_words])\n","    ), axis=1)[:, :top_sample]\n","\n","    word_list = list()\n","    count = 0\n","    for vector in df_text_vectors:\n","        tags = []\n","        for ind in closest_topn_index[count]:\n","            tags.append(dictionary_words[ind][\"label\"])\n","        word_list.append(tags)\n","        count += 1\n","\n","    #we obtain a list of lists, long as the sample itself\n","    return word_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ksKklbyZ0HFM"},"outputs":[],"source":["import pandas as pd\n","\n","#load data\n","#dataset source: https://www.kaggle.com/luthfim/steam-reviews-dataset\n","df = pd.read_csv('steam_reviews.csv')\n","if 'Unnamed: 0' in df.columns:\n","    df = df.drop('Unnamed: 0', axis=1)\n","df = df.dropna()\n","df = df.reset_index()\n","df.columns = ['_id']+list(df.columns)[1:]\n","df.rename(columns={'review': 'text'}, inplace=True)\n","df"]},{"cell_type":"markdown","metadata":{"id":"xNW5UCpz0HFN"},"source":["### ðŸ“ preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a8dlD8jt0HFN"},"outputs":[],"source":["df = df[['funny', 'helpful', 'hour_played', 'text', 'title', 'recommendation']]\n","df = df.dropna()\n","df = df.sample(10, random_state=35)\n","df['title'] = df['title'].apply(lambda x : x.lower())\n","\n","#encode\n","df['text_sentence_transformers_vector_'] = df['text'].apply(lambda x : model.encode(x))\n","\n","#df\n","df['zeroshot_list'] = zeroshot(df['text'], df['text_sentence_transformers_vector_'], model, 5000, 10)\n","df = pd.concat([df, pd.get_dummies(df['recommendation'])], axis=1)\n","df"]},{"cell_type":"markdown","metadata":{"id":"V4ddUn5i0HFO"},"source":["### ðŸ§© create list of dictionaries\n","\n","The only format we can upload to relevanceai is a list of dictionaries.\n","We need to convert our pandas dataframe into this format to proceed with our project. \n","Notice how we are converting our data into this format by using small batches. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XoeWSXMm0HFO"},"outputs":[],"source":["#convert to df_ready in batches\n","rows_ = list()\n","for rows in range(0, len(df), 20):\n","    rows_.append(rows)\n","rows_.append(len(df))\n","\n","df_ready = list()\n","for r in range(len(rows_)-1):\n","    #print(rows_[r], rows_[r+1])\n","    df_ready += df[rows_[r]:rows_[r+1]].to_dict(orient='records')\n","df_ready"]},{"cell_type":"markdown","metadata":{"id":"Jt40U1y30HFO"},"source":["## ðŸ§¬ encoding\n","\n","Because encoding is usually a long process, with a speed that is highly dependent on the encoder we choose, we might want to use a progressbar. \n","After our data will be encoded, a new field with the suffix \"sentence_transformers_vector_\" will be created for each sample.\n","After encoding, we will upload batches of our data using bulk_insert."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"IJAkSSrp0HFP","executionInfo":{"status":"ok","timestamp":1645624561535,"user_tz":-60,"elapsed":240,"user":{"displayName":"Michelangiolo Mazzeschi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11600464690802347668"}}},"outputs":[],"source":["def batch_splitting(len_df, range_len):\n","    range_list = list()\n","    if range_len >= len_df:\n","        range_list.append([0, len_df])\n","    else:\n","        for a in range(int(len_df/range_len)):\n","            range_list.append([a*range_len, (a+1)*range_len])\n","        range_list.append([range_list[-1][1], len_df])\n","    return range_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BAhn7s6W0HFP"},"outputs":[],"source":["from vectorhub.encoders.text.sentence_transformers import SentenceTransformer2Vec\n","import progressbar\n","import relevanceai\n","\n","#clean dataset, otherwise repeated clustering throws error\n","#client.datasets.delete(dataset_id=DATASET_ID) #in case we want a fresh start\n","batches = batch_splitting(len_df=len(df_ready), range_len=5000)\n","\n","bar = progressbar.ProgressBar(maxval=len(batches), widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n","model = SentenceTransformer2Vec(MODEL)\n","\n","#encoding\n","df_ready_encoded = list()\n","bar.start()\n","counter = 0\n","for batch in batches:\n","    bar.update(counter)\n","    current_vectors = model.encode_documents(documents=df_ready[batch[0]:batch[1]], fields=ENCODING_FIELDS)\n","    df_ready_encoded += current_vectors\n","    counter += 1\n","bar.finish()\n","\n","#we operate on df_ready\n","df_ready = df_ready_encoded\n","\n","#upload\n","bar.start()\n","counter = 0\n","for batch in batches:\n","    bar.update(counter)\n","    client.datasets.bulk_insert(dataset_id=DATASET_ID, documents=df_ready[batch[0]:batch[1]])\n","    counter += 1\n","bar.finish()"]},{"cell_type":"markdown","metadata":{"id":"GWyKSs9X0HFP"},"source":["## ðŸª„ clustering\n","\n","To perform a clustering on our data, we first need to calculate the centroids. The data will be uploaded to relevanceai.\n","Then, we will need to call another method that perform the clustering, with the alias containing the number of clusters we just used to find the centroids.\n","We can call this function with a different number of cluster as many times as we wish."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iKh6kGg40HFP"},"outputs":[],"source":["import relevanceai\n","\n","# Vector field based on which clustering is done - (Currently only one vector is supported)\n","vector_field = 'descriptiontextmulti_vector_'\n","\n","#calculate centroids\n","centroids = client.vector_tools.cluster.kmeans_cluster(\n","    dataset_id = DATASET_ID, \n","    #vector_fields=[[f'{x}' for x in ENCODING_FIELDS][0]], \n","    vector_fields=[x+VECTOR_SUFFIX for x in ENCODING_FIELDS], #potential bug when in our dataset we do not have a text field\n","    k = CLUSTERS)\n","\n","#creates clusters but only gives the centroids\n","#clustering results is uploaded on the database\n","\n","client.datasets.schema(DATASET_ID)\n","\n","client.services.cluster.centroids.list_closest_to_center(\n","  dataset_id=DATASET_ID,\n","  #vector_fields=[[f'{x}' for x in ENCODING_FIELDS][0]], \n","  vector_fields=[x+VECTOR_SUFFIX for x in ENCODING_FIELDS],\n","  page_size=40,\n","  #cluster_ids=[], # Leave this as an empty list if you want all of the clusters\n","  alias=f\"kmeans_{CLUSTERS}\" #change to 'kmeans_10' \n",")"]},{"cell_type":"markdown","metadata":{"id":"A-NmkKHH0HFQ"},"source":["## ðŸ¥¡ sharing\n","\n","It's time to share our app! By running the code below, you will be given a public link that you can share with your friends.\n","Know that by using the clustering dashboard once you login to relevanceai, you can manually edit the parameters of your clustering application."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5KPYcnAM0HFQ"},"outputs":[],"source":["import relevanceai\n","import requests\n","\n","response = requests.post(\n","    \"https://gateway-api-aueast.relevance.ai/latest/deployables/create\", \n","    json={\"dataset_id\":\n","            f\"{DATASET_ID}\",\"configuration\":\n","                {\n","                    \"collection_name\": f\"{DATASET_ID}\",\n","                    \"type\": \"cluster\", \n","                    \"deployable_name\": f\"{DATASET_ID}\", \n","                    \"project_id\": f\"{PROJECT_ID}\",\n","                    \"cluster\": {\n","                        \"alias\": f\"kmeans_{CLUSTERS}\",\n","                        #\"vector_field\": [f'{x}' for x in ENCODING_FIELDS][0]} #\n","                        \"vector_field\": [x+VECTOR_SUFFIX for x in ENCODING_FIELDS][0]}\n","            ,\"clusters-card-builder\": \n","                {\n","                    \"previewComponents\":\n","                        [{\n","                            \"previewType\": \"centroids\",\n","                            \"previewFields\": FIELDS, #any way to select all fields?\n","                            \"tabName\": \"\",\n","                            \"displayType\": \"default\"\n","                        },\n","                        AGG,\n","                        {}\n","                        ],\n","                    \"expandedComponents\":[]},\n","                    \"deployable_logo\":\n","                        None},\n","                    \"upsert\":\n","                        False},\n","    headers={\"Content-Type\":\"application/json\", \"Authorization\": \"michelangioloma:TDFvdm9Yd0J1M3VVTEcyQTV3VWs6b3RnRXJnUW9SQ2kwMVVJWVZ2VzVtZw\"}\n",")\n","\n","DEPLOYABLE_ID = response.json()['deployable_id']\n","\n","#\n","response = requests.post(\n","    f\"https://gateway-api-aueast.relevance.ai/latest//deployables/{DEPLOYABLE_ID}/share\", \n","    headers={\"Content-Type\":\"application/json\", \"Authorization\": \"michelangioloma:TDFvdm9Yd0J1M3VVTEcyQTV3VWs6b3RnRXJnUW9SQ2kwMVVJWVZ2VzVtZw\"}\n",")\n","response.json()\n","\n","#\n","response = requests.get(\n","    f\"https://gateway-api-aueast.relevance.ai/v2/deployables/{DEPLOYABLE_ID}/get\", \n","    headers={\"Content-Type\":\"application/json\", \"Authorization\": \"michelangioloma:TDFvdm9Yd0J1M3VVTEcyQTV3VWs6b3RnRXJnUW9SQ2kwMVVJWVZ2VzVtZw\"}\n",")\n","response.json()\n","\n","SHARABLE_API = response.json()['api_key']\n","SHARABLE_API\n","\n","with open(f'datasets/app_{DATASET_ID}.txt', 'a') as file:\n","    file.truncate(0)\n","    file.write(f'https://cloud.relevance.ai/dataset/{DATASET_ID}/deploy/cluster/{PROJECT_ID}/{SHARABLE_API}/{DEPLOYABLE_ID}')\n","\n","with open('app_url.txt', 'a') as file:\n","    file.write(f'https://cloud.relevance.ai/dataset/{DATASET_ID}/deploy/cluster/{PROJECT_ID}/{SHARABLE_API}/{DEPLOYABLE_ID}\\n')\n","\n","print('done')"]}],"metadata":{"interpreter":{"hash":"75a300ae82dd7b8f387c1777b66b2ec8c7a5f6d51d6392630ee9b10fab7f95f8"},"kernelspec":{"display_name":"Python 3.9.0 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"orig_nbformat":4,"colab":{"name":"notebook.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}
